{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:47:37.650501: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-09 21:47:38.140303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import resampy\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers, optimizers\n",
    "\n",
    "\n",
    "import params as yamnet_params\n",
    "import yamnet as yamnet_model\n",
    "import features as features_lib\n",
    "from random import shuffle\n",
    "\n",
    "import librosa\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:47:40.538366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-09 21:47:40.566396: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "intense = {\"약함\" : 0.,\"보통\":1.,\"강함\":2.}\n",
    "emotion_enc = {\"기쁨\" : 0, \"사랑스러움\" : 1, \"두려움\" : 2, \"화남\" : 3, \"슬픔\" : 4, \"놀라움\" : 5, \"없음\" : 6}\n",
    "emotion_dec = { 0 : \"기쁨\", 1 : \"사랑스러움\", 2 : \"두려움\", 3 : \"화남\", 4 : \"슬픔\", 5 : \"놀라움\", 6 : \"없음\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yamnet_params.Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segments_from_json(paths, wav_prefix):\n",
    "    for path in paths:\n",
    "        with open(path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        sr = int(data['Wav']['SamplingRate'])\n",
    "        tmp_prefix = os.path.join(wav_prefix, \"indoor\" if \"indoor\" in path else \"outdoor\")\n",
    "        wav_name = os.path.join(tmp_prefix, data['File']['FileName'] + \".wav\")\n",
    "        audio, _ = librosa.load(wav_name, sr=sr)\n",
    "        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=16000)\n",
    "        #y_emotion_class = []\n",
    "        #y_emotion_intensity = []\n",
    "        #audios = []\n",
    "\n",
    "        for entry in data['Conversation']:\n",
    "            start_time = float(entry['StartTime'].replace(\",\",\"\"))\n",
    "            end_time = float(entry['EndTime'].replace(\",\",\"\"))\n",
    "            emotion_category = emotion_enc[entry['VerifyEmotionTarget']]\n",
    "            emotion_intense = intense[entry['VerifyEmotionLevel']]\n",
    "            start_sample = int(start_time * 16000)\n",
    "            end_sample = int(end_time * 16000)\n",
    "            audio_segment = audio_resampled[start_sample:end_sample]\n",
    "\n",
    "            #audios.append(np.array(audio_segments))\n",
    "            #y_emotion_class.append(emotion_category)\n",
    "            #y_emotion_intensity.append(emotion_level)\n",
    "\n",
    "            yield np.array(audio_segment), emotion_category, emotion_intense\n",
    "\n",
    "        #return audios, np.array(y_emotion_class), np.array(y_emotion_intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../jspark/emotion/train\"\n",
    "audio_source = \"data\"\n",
    "label_str = \"label\"\n",
    "indoor_ = [\"indoor\", \"outdoor\"] \n",
    "indoor_files = glob.glob(os.path.join(path,label_str,indoor_[0], '*.json'))\n",
    "outdoor_files = glob.glob(os.path.join(path,label_str,indoor_[1], '*.json'))\n",
    "full_data = indoor_files + outdoor_files\n",
    "shuffle(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: load_segments_from_json(full_data, os.path.join(path,audio_source)),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(audio_segment, emotion_category, emotion_intensity):\n",
    "    waveform_padded = features_lib.pad_waveform(audio_segment, params)\n",
    "    #_, features = features_lib.waveform_to_log_mel_spectrogram_patches(\n",
    "    #    waveform_padded, params)\n",
    "    #num_patches = tf.shape(features)[1]\n",
    "    #tf.print(tf.shape(features))\n",
    "    #emotion_class_repeated = tf.repeat(emotion_category[tf.newaxis], num_patches, axis=0)\n",
    "    #emotion_intensity_repeated = tf.repeat(emotion_intensity[tf.newaxis], num_patches, axis=0)\n",
    "    labels = {\n",
    "        'emotion_class_output': emotion_category,\n",
    "        'emotion_intensity_output': emotion_intensity\n",
    "    }\n",
    "    #tf.print(tf.shape(emotion_class_repeated), len(labels[\"emotion_class_output\"]))\n",
    "    return audio_segment, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "batch_size = 32\n",
    "dataset = dataset.padded_batch(\n",
    "    batch_size,\n",
    "    padded_shapes=([None, ], {'emotion_class_output': [], 'emotion_intensity_output': []}),\n",
    "    padding_values=(0.0, {'emotion_class_output': 0, 'emotion_intensity_output': 0.0})\n",
    ")\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_model(input_shape, num_classes):\n",
    "    \"\"\"Defines the prediction model for emotion classification and intensity regression.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Dense(128, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    emotion_class_output = layers.Dense(num_classes, activation='softmax', name='emotion_class_output')(x)\n",
    "    emotion_intensity_output = layers.Dense(1, activation='linear', name='emotion_intensity_output')(x)\n",
    "    model = Model(inputs=inputs,  outputs= [emotion_class_output, emotion_intensity_output], name='emotion_recognition_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yamnet_params.Params()\n",
    "\n",
    "embedding_model = yamnet_model.yamnet_embedding_model(params)\n",
    "\n",
    "for layer in embedding_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "num_classes = len(emotion_enc)\n",
    "prediction_net = prediction_model(input_shape=(1024,), num_classes=num_classes)\n",
    "\n",
    "waveform_input = embedding_model.inputs[0]\n",
    "embeddings = embedding_model.outputs[0]\n",
    "predictions = prediction_net(embeddings)\n",
    "#waveform_input = layers.Input(shape=(None,), dtype=tf.float32)\n",
    "#embeddings = embedding_model(waveform_input)\n",
    "#predictions = prediction_net(embeddings)\n",
    "\n",
    "model = Model(inputs=waveform_input, outputs={\"emotion_class_output\": predictions[0],\n",
    "             \"emotion_intensity_output\": predictions[1]}, name='emotion_recognition_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "\n",
    "lr_schedule = optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=epoch\n",
    ")\n",
    "\n",
    "optimizer = optimizers.Adam(lr_schedule)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        'emotion_class_output': 'sparse_categorical_crossentropy',\n",
    "        'emotion_intensity_output': 'mean_squared_error'\n",
    "    },\n",
    "    metrics={\n",
    "        'emotion_class_output': 'accuracy',\n",
    "        'emotion_intensity_output': 'mae'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 21:47:22.123421: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-10-09 21:47:22.123690: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-10-09 21:47:33.258862: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 75 of 1000\n"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, epochs=epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('10epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yamnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
